# -*- coding: utf-8 -*-
"""Bert__project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LeSEOpwwliMUOBgV6twoX3K7tfkLOa0L

# TWO STEP CLASSIFICATION WSING RECASTED DATA FOR LOW REESOURCE LANGUAGE (HINDI)
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

#actual data set have 2 columns (context, label).consider Product review data set, 
#it contain 4 label (positive, negative, conflict, neutral)
#create new data set with help of all 8 possible of label like(not positive,positive)
#predict new data set accuracy using 4 model
#BoW, Sent2Vec, InferSent, XLM-RoBERTa

c

df.head(8)

import torch

import numpy as np
import torch
import pdb
import csv
from torch.nn.utils.rnn import pad_sequence

!pip install transformers -q

!pip install sentencepiece -q

from transformers import XLMRobertaModel, XLMRobertaTokenizer
tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')
model = XLMRobertaModel.from_pretrained('xlm-roberta-base')

def get_emb(sentence):
  hi_tokens = tokenizer.encode(sentence,add_special_tokens = True)
  input_ids = torch.tensor(hi_tokens).unsqueeze(0)
  with torch.no_grad():
    features = model(input_ids)[0]
  embeddings = features.squeeze(0)
  #f = xlmr.extract_features(hi_tokens)
  return embeddings

a = get_emb('हाथ में बहुत अच्छा लगता है , लेकिन अगर आप ने इसे ज्यादा जोर से पकड़ लिया तो यह टूट सकता है ।')

a

def extract_from_file(data_file, max_sents):
    nli_labels_to_int = {'entailed': 0, 'not-entailed': 1}
    sentiment_labels_to_int = {'positive': 0, 'negative': 1, 'neutral': 2, 'conflict': 3}
    data = {'context': [], 'hypoths': [], 'senti_lbls': [], 'nli_lbls': []}

    c = []
    h = []
    l = []
    s = []
    dataset = ''
    with open(data_file, "r",encoding = 'utf-8') as f:
        data1 = list(csv.reader(f, delimiter='\t'))
        for i, row in enumerate(data1):
            if(len(row)==0 or i==0):
                continue
            c.append(row[1])
            h.append(row[2])
            s.append(row[3])
            l.append(row[4])
    #print(s)

    assert len(l) == len(s), "%s: labels and source files are not same length"
  
    added_sents = 0
  
    for i in range(len(l)):
        nli_lbl = l[i].strip()
        senti_lbl = s[i].strip()
        ctxt = c[i].strip()
        hypoth = h[i].strip()
        #print(hypoth)
        if nli_lbl not in nli_labels_to_int:
            continue
      #print(nli_lbl)
      #print ("bad nli label: %s" % (nli_lbl))

        if(dataset=='review'):
            if senti_lbl not in sentiment_labels_to_int:
                continue
        #print(senti_lbl)
        #print ("bad sentiment label: %s" % (senti_lbl))
        

        if added_sents >= max_sents:
             continue

        nli_label = nli_labels_to_int[nli_lbl]
        #print(nli_label)
        if(dataset=='review'):
            senti_label = sentiment_labels_to_int[senti_lbl]
        else:
            senti_label = sentiment_labels_to_int[senti_lbl]
            #senti_label = (int)(senti_lbl)

        added_sents += 1

        data['context'].append(ctxt)
        data['nli_lbls'].append(nli_label)
        data['hypoths'].append(hypoth)  
        data['senti_lbls'].append(senti_label)
    
    return data

def get_nli_hypoth(train_data_file, val_data_file, test_data_file, max_train_sents, max_val_sents, max_test_sents):
    
    labels = {}
    hypoths = {}

    train_data = extract_from_file(train_data_file, max_train_sents)
    #print(train_data)
    train_length = len(train_data['nli_lbls'])
    

    val_data = extract_from_file(val_data_file, max_val_sents)
    val_length = len(val_data['nli_lbls'])

    test_data = extract_from_file(test_data_file, max_test_sents)
    test_length = len(test_data['nli_lbls'])
  
    data_train = {'nli_lbls': [], 'senti_lbls': [], 'context': [], 'hypoths': []}
  
    data_train['nli_lbls'] = train_data['nli_lbls']
    data_train['senti_lbls'] = train_data['senti_lbls']
    data_train['context'] = train_data['context']
    data_train['hypoths'] = train_data['hypoths']

    data_val = {'nli_lbls': [], 'senti_lbls': [], 'context': [], 'hypoths': []}
    data_val['nli_lbls'] = val_data['nli_lbls']
    data_val['senti_lbls'] = val_data['senti_lbls']
    data_val['context'] = val_data['context']
    data_val['hypoths'] = val_data['hypoths']

    data_test = {'nli_lbls': [], 'senti_lbls': [], 'context': [], 'hypoths': []}
    data_test['nli_lbls'] = test_data['nli_lbls']
    data_test['senti_lbls'] = test_data['senti_lbls']
    data_test['context'] = test_data['context']
    data_test['hypoths'] = test_data['hypoths']
    return data_train, data_val, data_test

def get_batch(batch, word_emb_dim):
    #embed = torch.zeros((len(batch), word_emb_dim))
    embed = []
    
    defined_length = 32
 
    for i in range(len(batch)):
        
        x = get_emb(batch[i])
        #print(batch[i])
        if(x.shape[0]<defined_length):
            pad = defined_length - x.shape[0]
            padded = torch.zeros(pad, x.shape[1])
            x = torch.cat([x, padded], dim=0)
        else:
            x = x[:defined_length, :]
    
        avg_pool = torch.mean(x, axis=0)
        #for j in range(768):
         #   embed[i][j] = avg_pool[j]
        embed.append(avg_pool)
    embed = pad_sequence(embed, batch_first=True, padding_value=0.)
  
    return torch.FloatTensor(embed)

def get_args():
  parser = argparse.ArgumentParser(description='Training NLI model for sent2vec embedding')

  # paths
  parser.add_argument("--outputdir", type=str, default='./saved_models/', help="Output directory")
  parser.add_argument("--outputmodelname", type=str, default='constraint.pickle')
  
  # data
  parser.add_argument("--train_data", type=str, default='./Product_Review_Dataset/recasted_train_with_negation.tsv', help="data file containing the context-hypothesis pair along with the nli label.")
  parser.add_argument("--val_data", type=str, default='./Product_Review_Dataset/recasted_dev_with_negation.tsv', help="data file containing the context-hypothesis pair along with the nli label.")
  parser.add_argument("--test_data", type=str, default='./Product_Review_Dataset/recasted_test_with_negation.tsv', help="data file containing the context-hypothesis pair along with the nli label.")
  parser.add_argument("--max_train_sents", type=int, default=10000000, help="Maximum number of training examples")
  parser.add_argument("--max_val_sents", type=int, default=10000000, help="Maximum number of validation/dev examples")
  parser.add_argument("--max_test_sents", type=int, default=10000000, help="Maximum number of test examples")
  
  # training
  parser.add_argument("--n_epochs", type=int, default=15)
  parser.add_argument("--n_classes_nli", type=int, default=2)
  parser.add_argument("--n_classes_clf", type=int, default=4)
  parser.add_argument("--batch_size", type=int, default=128)
  parser.add_argument("--dpout_model", type=float, default=0., help="encoder dropout")
  parser.add_argument("--dpout_fc", type=float, default=0., help="classifier dropout")
  parser.add_argument("--optimizer", type=str, default="adam,lr=0.001", help="adam or sgd,lr=0.1")
  parser.add_argument("--lrshrink", type=float, default=5., help="shrink factor for sgd")
  parser.add_argument("--decay", type=float, default=0.9, help="lr decay")
  parser.add_argument("--minlr", type=float, default=1e-5, help="minimum lr")
  
  parser.add_argument("--embedding_size", type=int, default=1024, help="sentence embedding size in the trained embedding model")
  parser.add_argument("--max_norm", type=int, default=5., help="maximum norm value")
  parser.add_argument("--reg_lambda", type=int, default=2., help="lambda for supervised loss")
  
  # gpu
  parser.add_argument("--gpu_id", type=int, default=0, help="GPU ID")
  parser.add_argument("--seed", type=int, default=1234, help="seed")
  parser.add_argument("--load_saved", type=bool, default=True, help="seed")

  #misc
  parser.add_argument("--verbose", type=int, default=1, help="Verbose output")

  params, _ = parser.parse_known_args()
  args = params

  return params

def get_ordered_batch(context, hypothesis, target,sentiment):
  #c_wo_n, h_wo_n, t_wo_n, s_wo_n = [], [], [], []
  c_w_n, h_w_n, t_w_n, s_w_n = [], [], [] ,[]
  for i in range(len(context)):
    c_w_n.append(context[i])
    h_w_n.append(hypothesis[i])
    t_w_n.append(target[i])
    s_w_n.append(sentiment[i])

  return c_w_n, h_w_n, t_w_n, s_w_n

import re

import inspect



def get_optimizer(s):
    if "," in s:
        method = s[:s.find(',')]
        optim_params = {}
        for x in s[s.find(',') + 1:].split(','):
            split = x.split('=')
            assert len(split) == 2
            assert re.match("^[+-]?(\d+(\.\d*)?|\.\d+)$", split[1]) is not None
            optim_params[split[0]] = float(split[1])
    else:
        method = s
        optim_params = {}
        
    if method == 'adadelta':
        optim_fn = optim.Adadelta
    elif method == 'adagrad':
        optim_fn = optim.Adagrad
    elif method == 'adam':
        optim_fn = optim.Adam
    elif method == 'adamax':
        optim_fn = optim.Adamax
    elif method == 'asgd':
        optim_fn = optim.ASGD
    elif method == 'rmsprop':
        optim_fn = optim.RMSprop
    elif method == 'rprop':
        optim_fn = optim.Rprop
    elif method == 'sgd':
        optim_fn = optim.SGD
        if 'lr' not in optim_params:
            raise Exception('Missing required "lr" parameter for SGD optimizer')
    else:
        raise Exception('Unknown optimization method: "%s"' % method)
        
    expected_args = inspect.signature(optim_fn.__init__).parameters.keys()
    expected_args = list(expected_args)[2:]
    
    return optim_fn, optim_params

nli_net = NLI_Classification_Net(4)

optim_fn

lr = optim_params['lr'] if 'sgd' in args.optimizer else None

type(lr)

optimizer = optim_fn(nli_net.parameters(), **optim_params)

  global val_acc_best, lr, stop_training, adam_stop
  val_acc_best = -1e10
  adam_stop = False
  stop_training = False
  

  epoch = 1

def get_optimizer(s):
  if "," in s:
    method = s[:s.find(',')]
    optim_params ={}
    for x in s[s.find(',')+ 1:].split(','):
      split = x.split('=')
      assert len(split) == 2
      assert re.match("^[+-]?(\d+(\.\d*)?|\.\d+)$",split[1]) is not None
      optim_params[split[0]] = float(split[1])
  else:
    method = s
    optim_params = {}
    if method == 'adadelta':
      optim_fn = optim.Adadelta
    elif method == 'adagrad':
      optim_fn = optim.Adagrad
    elif method == 'adam':
      optim_fn = optim.Adam
    elif method == 'adamax':
      optim_fn = optim.Adamax
    elif method == 'asgd':
      optim_fn = optim.ASGD
    elif method == 'rmsprop':
      optim_fn = optim.RMSprop
    elif method == 'rprop':
      optim_fn = optim.Rprop
    elif method == 'sgd':
      optim_fn = optim.SDG
      assert 'lr' in optim_params
    else:
      raise Exception('Unknown optimization method: "%s"'% method)
    expected_args = inspect.signature(optim_fn.__init__).parameters.keys()
    expected_args= list(expected_args)[2:]
    return optim_fn, optim_params

optim_fn, optim_params = get_optimizer(args.optimizer)

if torch.cuda.is_available():
  device = torch.device("cuda")  # Note lowercase "cuda"
  print("GPU")
else:
  device = torch.device("cpu")  # Note lowercase "cpu"
  print("CPU")

def trainepoch(epoch, train , optimizer, args, nli_net, loss_fn, loss_mse):
  nli_net.train()
  print('\nTRAINING : Epoch ' + str(epoch))
  
  nli_net.train()
  all_costs = []
  logs = []
  
  last_time = time.time()
  correct = 0.
  
  # shuffle the data
  permutation = np.random.permutation(len(train['hypoths']))
  context, hypoths, target,sentiment = train['context'], train['hypoths'], train['nli_lbls'],train['senti_lbls']

  optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * args.decay if epoch>1\
      and 'sgd' in args.optimizer else optimizer.param_groups[0]['lr']
  print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))


  trained_sents = 0
  counter = 1

  start_time = time.time()
  for stidx in range(0, len(hypoths), args.batch_size):
    # prepare batch
    c_w_n, h_w_n, t_w_n,s_w_n = get_ordered_batch(context[stidx:stidx + args.batch_size], hypoths[stidx:stidx + args.batch_size], target[stidx:stidx + args.batch_size], sentiment[stidx:stidx + args.batch_size])
    
    c_w_n =  get_batch(c_w_n, args.embedding_size)
    h_w_n =  get_batch(h_w_n, args.embedding_size)
    
    tgt_batch = None
    
    s = []
    for i in range(len(s_w_n)):
      if i % 8 == 0:
        s.append(s_w_n[i])
    #print(type(s))
    #print(s)
    c_w_n =  Variable(c_w_n).to(device)
    h_w_n =  Variable(h_w_n).to(device)
    t_w_n = Variable(torch.LongTensor(t_w_n)).to(device)
    s = Variable(torch.LongTensor(s)).to(device)
    k = 2 * t_w_n.shape[0]
    
    # model forward
   # premise_reduced_batch = torch.zeros(8)
   # k = 0
    #for i in range(s_w_n.shape[0]):
    #  if(i%8 ==0):
    #    premise_reduced_batch[k] = s_w_n[i]

     #   k += 1
 
# Print the resulting tensor
    #print(result_tensor)

    #op_wo_vector = nli_net(c_wo_n, h_wo_n)
    op_nli_vector, op_senti_vector = nli_net(c_w_n, h_w_n)
    #print(op_wo_vector)
    #print(op_w_vector)
    #p_wo_vector = torch.sigmoid(op_wo_vector)
    p_w_vector = torch.sigmoid(op_senti_vector)

    #print(p_w_vector)
    pred = torch.argmax(op_senti_vector, dim=1)
    #pred1 = torch.argmax(p_w_vector, dim=1)
    print(pred)
    #pred(pred1)
    #print(premise_reduced_batch)
    print(s)
    
    correct += pred.long().eq(s.data.long()).float().to(device).sum()
    assert len(pred) == len(s)

    supervised_loss_senti = loss_fn(op_senti_vector, s)
    supervised_loss_nli = loss_fn(op_nli_vector, t_w_n)

    #constraint_loss = loss_mse(p_w_vector, 1-p_w_vector)

    loss = args.reg_lambda * supervised_loss_senti + supervised_loss_nli

    all_costs.append(loss.item())
    
    loss.backward()

    shrink_factor = 0.9
    total_norm = 0
    for p in nli_net.parameters():
      if p.requires_grad:
        p.grad.data.div_(k)
        total_norm += p.grad.data.norm() ** 2
    total_norm = np.sqrt(total_norm.cpu().data)

    if total_norm > args.max_norm:
        shrink_factor = args.max_norm / total_norm

    if('sgd' in args.optimizer):
      current_lr = optimizer.param_groups[0]['lr']
      optimizer.param_groups[0]['lr'] = current_lr * shrink_factor

    optimizer.step()
    
    if('sgd' in args.optimizer):
      optimizer.param_groups[0]['lr'] = current_lr

    if args.verbose:
      trained_sents += len(s)
      counter += 1

      print("supervised_loss_sentiment: ", supervised_loss_senti.item())
      print("supervised_loss_TE: ", supervised_loss_nli.item())
      #print("constraint_loss: ", constraint_loss.item())
      print("Total_loss: ", loss.item())
      print ("epoch: %d -- correct %d / %d trained  ----- accuracy %d " % (epoch, correct,(trained_sents), ((correct * 100 ) / trained_sents).item()))
      
  train_acc = (100 * correct / (len(hypoths)/8)).item()
  print('results : epoch {0} ; mean accuracy train : {1}, loss : {2}'.format(epoch, train_acc, round(np.mean(all_costs), 2)))
  
  return train_acc, nli_net

def evaluate(epoch, valid, optimizer, args, nli_net, eval_type='valid', final_eval=False):
  nli_net.eval()
  correct = 0.
  global val_acc_best, lr, stop_training, adam_stop

  if eval_type == 'valid':
    print('\n{0} : Epoch {1}'.format(eval_type, epoch))

  context = valid['context']
  hypoths = valid['hypoths']
  target = valid['nli_lbls']
  sentiment = valid['senti_lbls']

  
  for i in range(0, len(hypoths), args.batch_size):
    
    context_batch = get_batch(context[i:i + args.batch_size], args.embedding_size)
    hypoths_batch = get_batch(hypoths[i:i + args.batch_size], args.embedding_size)
    senti_batch = sentiment[i:i + args.batch_size]
    s = []
    for i in range(len(senti_batch)):
      if i % 8 == 0:
        s.append(senti_batch[i])
        
    tgt_batch = None
    
    context_batch = Variable(context_batch).to(device)
    hypoths_batch = Variable(hypoths_batch).to(device)
    tgt_batch = Variable(torch.LongTensor(s)).to(device)

    output = nli_net(context_batch, hypoths_batch)
    pred = torch.argmax(output, dim=1)
    correct += pred.long().eq(tgt_batch.data.long()).float().to(device).sum()

  ###### save model
  eval_acc = (100 * correct / (len(hypoths)/8)).item()
  
  if final_eval:
    print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))
  else:
    print('togrep : results : epoch {0} ; mean accuracy {1} :\
              {2}'.format(epoch, eval_type, eval_acc))

  if eval_type == 'valid' and epoch <= args.n_epochs:
    if eval_acc > val_acc_best:
      print('saving model at epoch {0}'.format(epoch))
      if not os.path.exists(args.outputdir):
        os.makedirs(args.outputdir)
      torch.save(nli_net, os.path.join(args.outputdir, args.outputmodelname))
      val_acc_best = eval_acc
    else:
      if 'sgd' in args.optimizer:
        optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / args.lrshrink
        if optimizer.param_groups[0]['lr'] < args.minlr:
          stop_training = True
      if 'adam' in args.optimizer:
        stop_training = adam_stop
        adam_stop = True
  return eval_acc

import os
import sys
import time
import argparse
import pdb

args = get_args()

def main(args):

  np.random.seed(args.seed)
  torch.manual_seed(args.seed)
  


  train_df,dev_df,test_df = get_nli_hypoth('/content/drive/MyDrive/pr_train.tsv','/content/drive/MyDrive/pr_val.tsv','/content/drive/MyDrive/pr_test.tsv', max_train_sents=26616, max_val_sents=4008, max_test_sents=4000)
  #print(train_df)
  nli_net = NLI_Classification_Net(4)
  
  # loss
  loss_fn = nn.CrossEntropyLoss()
  loss_mse = nn.MSELoss()
  # optimizer
  optim_fn, optim_params = get_optimizer(args.optimizer)
  optimizer = optim_fn(nli_net.parameters(), **optim_params)

  global val_acc_best, lr, stop_training, adam_stop
  val_acc_best = -1e10
  adam_stop = False
  stop_training = False
  lr = optim_params['lr'] if 'sgd' in args.optimizer else None

  epoch = 1



  while not stop_training and epoch <= args.n_epochs:
    train_acc, nli_net = trainepoch(epoch, train_df, optimizer, args, nli_net, loss_fn, loss_mse)
    eval_acc = evaluate(epoch, dev_df, optimizer, args, nli_net, 'valid')
    epoch += 1

  nli_net = torch.load(os.path.join(args.outputdir, args.outputmodelname))

  print("The Tests Accuracy is ",evaluate("NO", test_df, args , nli_net , "test"))

main(args)

class NLI_Reverse_Net_Exact(nn.Module):
    def __init__(self, n_classes):
        super(NLI_Reverse_Net_Exact, self).__init__()

        self.n_classes = n_classes
        self.nli_classes = 2

        self.reverse_classifier = nn.Sequential(
            nn.Linear(self.nli_classes*self.n_classes, 8),
            nn.Linear(8, self.n_classes)
            )

    def forward(self, e_emb):
        output = self.reverse_classifier(e_emb)
        return output

import numpy as np
import time
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F

class NLI_Classification_Net(nn.Module):
    def __init__(self, n_classes):
        super(NLI_Classification_Net, self).__init__()

        self.nli_classes = 2
        self.sentiment_classes = n_classes

        self.softmax = nn.Softmax(dim=1)

        self.nli_classifier = nn.Sequential(
            nn.Linear(4*768, 512),
            nn.ReLU(),
            nn.Linear(512, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, self.nli_classes),
            )

        self.sentiment_classifier = nn.Sequential(
            nn.Linear(self.nli_classes*self.sentiment_classes*2 + 768, 8),
            nn.Linear(8, self.sentiment_classes)
            )

        self.to(device)
    def forward(self, c_emb, h_emb):
        c1 = c_emb
        c2 = h_emb
        
        c3 = torch.abs(c_emb-h_emb)
        c4 =c_emb*h_emb

        features = torch.cat([c1, c2], dim=1)
        features = torch.cat([features, c3], dim=1)
        features = torch.cat([features, c4], dim=1)
        features = features.view(features.shape[0], -1)
        
        nli_output = self.nli_classifier(features)
        nli_probs = self.softmax(nli_output)
        nli_probs_1 = nli_probs.view(nli_probs.shape[0] // (self.nli_classes * self.sentiment_classes), (self.nli_classes * self.sentiment_classes) * nli_probs.shape[1])
        
        premise_reduced_batch = torch.zeros(nli_probs_1.shape[0], 768)
        k = 0
        for i in range(c_emb.shape[0]):
            if(i%(self.nli_classes * self.sentiment_classes)==0):
                premise_reduced_batch[k, :] = c_emb[i, :]
                k += 1
        concat_input = torch.cat([premise_reduced_batch.to(device), nli_probs_1], dim=1)
        clf_output = self.sentiment_classifier(concat_input)
        return nli_output, clf_output

class NLI_Classification_Net(nn.Module):
    def __init__(self, n_classes):
        super(NLI_Classification_Net, self).__init__()

        self.nli_classes = 2
        self.sentiment_classes = n_classes
        
        self.softmax = nn.Softmax(dim=1)
        self.n_classes = 4
        self.nli_classifier = nn.Sequential(
            nn.Linear(4*768, 512),
            nn.ReLU(),
            nn.Linear(512, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, self.nli_classes),
            )


    def forward(self, c_emb, h_emb):
        c1 = c_emb
        c2 = h_emb
        
        c3 = torch.abs(c_emb-h_emb)
        c4 =c_emb*h_emb
        print(c4)
        features = torch.cat([c1, c2], dim=1)
        features = torch.cat([features, c3], dim=1)
        features = torch.cat([features, c4], dim=1)
        #features = features.view(features.shape[0], -1)
        
        nli_output = self.nli_classifier(features)

        return  nli_output

class NLI_Classification_Net(nn.Module):
    def __init__(self, n_classes):
        super(NLI_Classification_Net, self).__init__()

        self.nli_classes = 2
        self.sentiment_classes = n_classes
        
        self.softmax = nn.Softmax(dim=1)
        self.n_classes = 4
        self.nli_classifier = nn.Sequential(
            nn.Linear(4*768, 512),
            nn.ReLU(),
            nn.Linear(512, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, self.nli_classes),
            )

        self.sentiment_classifier = nn.Sequential(
            nn.Linear(2*2 + 768, 8),
            nn.Linear(8, self.sentiment_classes)
            )


    def forward(self, c_emb, h_emb):
        c1 = c_emb
        c2 = h_emb
        
        c3 = torch.abs(c_emb-h_emb)
        c4 =c_emb*h_emb

        features = torch.cat([c1, c2], dim=1)
        features = torch.cat([features, c3], dim=1)
        features = torch.cat([features, c4], dim=1)
        features = features.view(features.shape[0], -1)
        print(features)
        nli_output = self.nli_classifier(features)
        nli_probs = self.softmax(nli_output)
        nli_probs_1 = nli_probs.view(nli_probs.shape[0] // (2), (2) * nli_probs.shape[1])
        
        premise_reduced_batch = torch.zeros(nli_probs_1.shape[0], 768)
        k = 0
        for i in range(c_emb.shape[0]):
            if(i%(2)==0):
                premise_reduced_batch[k, :] = c_emb[i, :]
                k += 1
        concat_input = torch.cat([premise_reduced_batch, nli_probs_1], dim=1)
        clf_output = self.sentiment_classifier(concat_input)
        return clf_output



dev = pd.read_table('PR_recasted_dev.tsv')

dev['Sentiment_Label'][6]

for i in range(4328):
    if dev['Entailment_Label'][i] == 'not-entailed':
        dev['Entailment_Label'][i] = 1
    else:
        dev['Entailment_Label'][i] = 0

for i in range(4328):
    if dev['Sentiment_Label'][i] == 'positive':
        dev['Sentiment_Label'][i] = 0
    elif dev['Sentiment_Label'][i] == 'negative':
        dev['Sentiment_Label'][i] = 1
    elif dev['Sentiment_Label'][i] == 'neutral' :
        dev['Sentiment_Label'][i] = 2
    else:
        dev['Sentiment_Label'][i] = 3

dev['Context'][0]

dev.head(10)

"""# WORD EMBEDDING """

from fairseq.models.roberta import XLMRModel

xlmr = XLMRModel.from_pretrained('xlmr.base.tar.gz', checkpoint_file='model.pt')

xlmr.eval()

def get_emb(sentence):
    hi_tokens = xlmr.encode(sentence)
    f = xlmr.extract_features(hi_tokens)
    return f.squeeze(0)

a = get_emb('हाथ में बहुत अच्छा लगता है , लेकिन अगर आप ने इसे ज्यादा जोर से पकड़ लिया तो यह टूट सकता है ।')

a.shape

#for i in range(4328):
b = get_batch('हाथ में बहुत अच्छा लगता है , लेकिन अगर आप ने इसे ज्यादा जोर से पकड़ लिया तो यह टूट सकता है ।',1024)

b.shape

for i in range(4328):
    dev['Hypothesis'][i] = get_emb(dev['Hypothesis'][i])

embed = torch.zeros((2, 5))
embed[0][3] = 6

embed.

dev







test = pd.read_table('PR_recasted_test.tsv')
train = pd.read_table('PR_recasted_train.tsv')



type(train_df)

a = []
for i in range(4328):
    a.append(dev['Context'][i])
print(a)

len(a)

for i in a:
    print(get_batch(i, 1024))

"""# using bert for predict the actual label

"""

import numpy as np
import time
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F

"""# Textual_Entailment"""

class NLI_HYPOTHS_Net(nn.Module):
    def __init__(self):
        super(NLI_HYPOTHS_Net, self).__init__()

        self.n_classes = 1

        self.classifier = nn.Sequential(
            nn.Linear(4*1024, 256),
            nn.Tanh(),
            nn.Linear(256, 64),
            nn.Tanh(),
            nn.Linear(64, 16),
            nn.Tanh(),
            nn.Linear(16, 8),
            nn.Tanh(),
            nn.Linear(8, self.n_classes),
            )

    def forward(self, c_emb, h_emb):
        c1 = c_emb
        c2 = h_emb
        
        c3 = torch.abs(c_emb-h_emb)
        c4 = c_emb*h_emb

        features = torch.cat([c1, c2], dim=1)
        features = torch.cat([features, c3], dim=1)
        features = torch.cat([features, c4], dim=1)
        features = features.view(features.shape[0], -1)
        output = self.classifier(features)
        
        return output

class NLI_Reverse_Net(nn.Module):
    def __init__(self, n_classes):
        super(NLI_Reverse_Net, self).__init__()

        self.n_classes = n_classes
        self.nli_classes = 2

        self.reverse_classifier = nn.Sequential(
            nn.Linear(self.nli_classes*self.n_classes*2, 8),
            nn.Linear(8, self.n_classes)
            )

    def forward(self, e_emb):
        output = self.reverse_classifier(e_emb)
        return output

class NLI_Classification_Net(nn.Module):
    def __init__(self, n_classes):
        super(NLI_Classification_Net, self).__init__()

        self.nli_classes = 2
        self.sentiment_classes = n_classes

        self.softmax = nn.Softmax(dim=1)

        self.nli_classifier = nn.Sequential(
            nn.Linear(4*768, 512),
            nn.ReLU(),
            nn.Linear(512, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, self.nli_classes),
            )

        self.sentiment_classifier = nn.Sequential(
            nn.Linear(self.nli_classes*self.sentiment_classes*2 + 768, 8),
            nn.Linear(8, self.sentiment_classes)
            )


    def forward(self, c_emb, h_emb):
        c1 = c_emb
        c2 = h_emb
        
        c3 = torch.abs(c_emb-h_emb)
        c4 =c_emb*h_emb

        features = torch.cat([c1, c2], dim=1)
        features = torch.cat([features, c3], dim=1)
        features = torch.cat([features, c4], dim=1)
        features = features.view(features.shape[0], -1)
        
        nli_output = self.nli_classifier(features)
        nli_probs = self.softmax(nli_output)
        nli_probs_1 = nli_probs.view(nli_probs.shape[0] // (self.nli_classes * self.sentiment_classes), (self.nli_classes * self.sentiment_classes) * nli_probs.shape[1])
        
        premise_reduced_batch = torch.zeros(nli_probs_1.shape[0], 768)
        k = 0
        for i in range(c_emb.shape[0]):
            if(i%(self.nli_classes * self.sentiment_classes)==0):
                premise_reduced_batch[k, :] = c_emb[i, :]
                k += 1
        concat_inpput = torch.cat([premise_reduced_batch.cuda(), nli_probs_1], dim=1)
        clf_output = self.sentiment_classifier(concat_input)
        return nli_output, clf_output

class NLI_Reverse_Net_Exact(nn.Module):
    def __init__(self, n_classes):
        super(NLI_Reverse_Net_Exact, self).__init__()

        self.n_classes = n_classes
        self.nli_classes = 2

        self.reverse_classifier = nn.Sequential(
            nn.Linear(self.nli_classes*self.n_classes, 8),
            nn.Linear(8, self.n_classes)
            )

    def forward(self, e_emb):
        output = self.reverse_classifier(e_emb)
        return output

import os
import sys
import time
import argparse
import pdb

import numpy as np

import torch
from torch.autograd import Variable
import torch.nn as nn

#from model_data import get_nli_hypoth, get_batch
#from nli_model import NLI_HYPOTHS_Net
#from mutils import get_optimizer
import torch.nn.functional as F

def get_args():
    parser = argparse.ArgumentParser(description='Training NLI model for sent2vec embedding')

  # paths
    parser.add_argument("--outputdir", type=str, default='./saved_models/', help="Output directory")
    parser.add_argument("--outputmodelname", type=str, default='constraint.pickle')
  
  # data
    parser.add_argument("--train_data", type=str, default='./sentiment_data/Product_Review_Dataset/recasted_train_with_negation.tsv', help="data file containing the context-hypothesis pair along with the nli label.")
    parser.add_argument("--val_data", type=str, default='./sentiment_data/Product_Review_Dataset/recasted_dev_with_negation.tsv', help="data file containing the context-hypothesis pair along with the nli label.")
    parser.add_argument("--test_data", type=str, default='./sentiment_data/Product_Review_Dataset/recasted_test_with_negation.tsv', help="data file containing the context-hypothesis pair along with the nli label.")
    parser.add_argument("--max_train_sents", type=int, default=10000000, help="Maximum number of training examples")
    parser.add_argument("--max_val_sents", type=int, default=10000000, help="Maximum number of validation/dev examples")
    parser.add_argument("--max_test_sents", type=int, default=10000000, help="Maximum number of test examples")
  
  # training
    parser.add_argument("--n_epochs", type=int, default=10)
    parser.add_argument("--n_classes", type=int, default=2)
    parser.add_argument("--n_sentiment", type=int, default=4)
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--dpout_model", type=float, default=0., help="encoder dropout")
    parser.add_argument("--dpout_fc", type=float, default=0., help="classifier dropout")
    parser.add_argument("--optimizer", type=str, default="adam,lr=0.0001", help="adam or sgd,lr=0.1")
    parser.add_argument("--lrshrink", type=float, default=5., help="shrink factor for sgd")
    parser.add_argument("--decay", type=float, default=0.99, help="lr decay")
    parser.add_argument("--minlr", type=float, default=1e-5, help="minimum lr")
    parser.add_argument("--is_cr", type=bool, default=True, help="whether or not to include constranit regularization while training")
  
    parser.add_argument("--embedding_size", type=int, default=1024, help="sentence embedding size in the trained embedding model")
    parser.add_argument("--max_norm", type=int, default=5., help="maximum norm value")
  
  # gpu
    parser.add_argument("--gpu_id", type=int, default=0, help="GPU ID")
    parser.add_argument("--seed", type=int, default=1234, help="seed")
    parser.add_argument("--load_saved", type=bool, default=True, help="seed")

  #misc
    parser.add_argument("--verbose", type=int, default=1, help="Verbose output")

    params, _ = parser.parse_known_args()
    args = params

    return params

parser.add_argument("--verbose", type=int, default=1, help="Verbose output")

params, _ = parser.parse_known_args()
args = params

"""# function for accuracy and loss"""

def get_ordered_batch(context, hypothesis, target):
    c_wo_n, h_wo_n, t_wo_n = [], [], []
    c_w_n, h_w_n, t_w_n = [], [], []
  
    for i in range(len(context)):
        if(i%2==0):
            c_wo_n.append(context[i])
            h_wo_n.append(hypothesis[i])
            t_wo_n.append(target[i])
        else:
            c_w_n.append(context[i])
            h_w_n.append(hypothesis[i])
            t_w_n.append(target[i])

    return c_wo_n, h_wo_n, t_wo_n, c_w_n, h_w_n,

dev['Hypothesis'][0]

def trainepoch(epoch, train, optimizer, args, nli_net, loss_fn, loss_mse):
    nli_net.train()
    print('\nTRAINING : Epoch ' + str(epoch))
  # bhaav: adam 0.01
    nli_net.train()
    all_costs = []
    logs = []
  
    last_time = time.time()
    correct = 0.
  
  # shuffle the data
    permutation = np.random.permutation(len(train['hypoths']))
    context, hypoths, target = train['context'], train['hypoths'], train['nli_lbls']

    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * args.decay if epoch>1\and 'sgd' in args.optimizer else optimizer.param_groups[0]['lr']
    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))

    trained_sents = 0
    counter = 1

    start_time = time.time()
    for stidx in range(0, len(hypoths), args.batch_size):
        #prepare batch
         c_wo_n, h_wo_n, t_wo_n, c_w_n, h_w_n, t_w_n = get_ordered_batch(context[stidx:stidx + args.batch_size], hypoths[stidx:stidx + args.batch_size], target[stidx:stidx + args.batch_size])
    
         c_wo_n, c_w_n = get_batch(c_wo_n, args.embedding_size), get_batch(c_w_n, args.embedding_size)
         h_wo_n, h_w_n = get_batch(h_wo_n, args.embedding_size), get_batch(h_w_n, args.embedding_size)
    
         tgt_batch = None

         if args.gpu_id > -1:
                c_wo_n, c_w_n = Variable(c_wo_n.cuda()), Variable(c_w_n.cuda())
                h_wo_n, h_w_n = Variable(h_wo_n.cuda()), Variable(h_w_n.cuda())
                t_wo_n, t_w_n = Variable(torch.LongTensor(t_wo_n).cuda()), Variable(torch.LongTensor(t_w_n).cuda())
        else:
                c_wo_n, c_w_n = Variable(c_wo_n), Variable(c_w_n)
                h_wo_n, h_w_n = Variable(h_wo_n), Variable(h_w_n)
                t_wo_n, t_w_n = Variable(torch.LongTensor(t_wo_n)), Variable(torch.LongTensor(t_w_n))

        k = 2 * t_wo_n.shape[0]

    # model forward
    optimizer.zero_grad()

    op_wo_vector = nli_net(c_wo_n, h_wo_n)
    op_w_vector = nli_net(c_w_n, h_w_n)

    p_wo_vector = torch.sigmoid(op_wo_vector)
    p_w_vector = torch.sigmoid(op_w_vector)

    pred = torch.argmax(op_wo_vector, dim=1)
      
    correct += pred.long().eq(t_wo_n.data.long()).float().to(device).sum()
    assert len(pred) == len(h_wo_n)

    supervised_loss = loss_fn(op_wo_vector, t_wo_n)
    if(args.is_cr):
        constraint_loss = loss_mse(p_wo_vector, torch.ones_like(p_wo_vector)-p_w_vector)
        loss = 2 * supervised_loss + constraint_loss
    else:
        loss = supervised_loss

    all_costs.append(loss.item())
    
    loss.backward()

    shrink_factor = 0.9
    total_norm = 0

    for p in nli_net.parameters():
        if p.requires_grad:
            p.grad.data.div_(k)
            total_norm += p.grad.data.norm() ** 2
            total_norm = np.sqrt(total_norm.to(device).data)

    if total_norm > args.max_norm:
        shrink_factor = args.max_norm / total_norm

    if('sgd' in args.optimizer):
        current_lr = optimizer.param_groups[0]['lr']
        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor
        optimizer.step()
    
    if('sgd' in args.optimizer):
        optimizer.param_groups[0]['lr'] = current_lr

    if args.verbose:
        trained_sents += len(t_wo_n)
        counter += 1
    print("supervised_loss: ", supervised_loss.item())
    if(args.is_cr):
        print("constraint_loss: ", constraint_loss.item())
        print("Total_loss: ", loss.item())
        print ("epoch: %d -- correct %d / %d trained  ----- accuracy %d " % (epoch, correct, trained_sents, ((correct * 100 ) / trained_sents).item()))
      
    train_acc = (200 * correct / len(hypoths)).item()
    print('results : epoch {0} ; mean accuracy train : {1}, loss : {2}'
          .format(epoch, train_acc, round(np.mean(all_costs), 2)))
  
    return train_acc, nli_net

"""# main function"""